Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.
11:46:19-INFO: device: cuda n_gpu: 2, distributed training: False, 16-bits training: False
11:46:19-INFO: loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ha042/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
11:46:22-INFO: loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /home/ha042/.pytorch_pretrained_bert/distributed_-1/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba
11:46:22-INFO: extracting archive file /home/ha042/.pytorch_pretrained_bert/distributed_-1/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpv6jm_nts
11:46:33-INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

11:46:36-INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
11:46:36-INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
11:46:55-INFO: loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /home/ha042/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba
11:46:55-INFO: extracting archive file /home/ha042/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmp83gzonxe
11:47:06-INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

11:47:08-INFO: ***** Running evaluation *****
11:47:08-INFO:   Num examples = 4561
11:47:08-INFO:   Batch size = 32
Evaluating: 100%|█████████████████████████████| 143/143 [00:23<00:00,  6.07it/s]
11:47:32-INFO: ***** Eval results *****
11:47:32-INFO:   Accuracy = 0.6424029818022363
11:47:32-INFO:   eval_accuracy = 0.6424029818022363
11:47:32-INFO:   eval_loss = 0.9142084832374866
11:47:32-INFO:   global_step = 0
11:47:32-INFO:   inference_time = 23.350437879562378
11:47:32-INFO:   loss = None
0.6424029818022363

Layer \textbackslash~Head & 1 & 2 & 3 & 4 \\
1 & 0.00044 & -0.00307 & -0.00241 & -0.00219 & 0.00088 & -0.00329 & -0.00110 & -0.00044 & -0.00570 & 0.00132 & 0.00066 & 0.00044 \\
2 & -0.00022 & 0.00132 & 0.00088 & -0.00110 & -0.00088 & -0.00307 & 0.00066 & -0.00285 & 0.00044 & -0.00066 & 0.00088 & -0.00022 \\
3 & 0.00044 & -0.00110 & -0.00263 & -0.00373 & -0.00022 & 0.00000 & -0.00658 & -0.00088 & -0.00066 & 0.00022 & -0.00066 & 0.00175 \\
4 & -0.00175 & -0.00044 & -0.00044 & -0.00153 & -0.00088 & -0.00022 & -0.00088 & -0.00175 & 0.00022 & -0.00175 & -0.00373 & -0.00132 \\
5 & 0.00000 & -0.00044 & -0.00460 & 0.00022 & 0.00044 & 0.00000 & -0.00088 & 0.00219 & -0.00066 & 0.00132 & 0.00000 & 0.00110 \\
6 & 0.00175 & 0.00022 & 0.00175 & 0.00088 & 0.00110 & -0.00241 & -0.00153 & 0.00110 & -0.00197 & -0.00066 & -0.00219 & 0.00197 \\
7 & 0.00285 & -0.00088 & -0.00110 & -0.00022 & -0.00175 & 0.00088 & -0.00066 & -0.00132 & -0.00307 & -0.00066 & -0.00132 & -0.00088 \\
8 & 0.00066 & -0.00088 & -0.00022 & -0.00022 & 0.00110 & -0.00088 & 0.00000 & 0.00088 & -0.00044 & 0.00066 & -0.00022 & -0.00241 \\
9 & -0.00263 & -0.00044 & -0.00044 & -0.00241 & 0.00153 & -0.00197 & -0.00022 & -0.00175 & -0.00066 & 0.00000 & -0.00153 & 0.00022 \\
10 & -0.00132 & -0.00153 & 0.00066 & 0.00000 & -0.00285 & -0.00110 & -0.00022 & 0.00000 & 0.00767 & -0.00022 & -0.00175 & -0.02039 \\
11 & 0.00175 & -0.00197 & -0.00022 & -0.00417 & 0.00022 & 0.00022 & -0.00110 & 0.00022 & -0.00395 & -0.00241 & -0.01535 & 0.00044 \\
12 & -0.00460 & 0.00197 & 0.00789 & 0.00197 & 0.00044 & -0.00614 & -0.00504 & 0.00000 & 0.00110 & -0.00022 & 0.00132 & -0.00636 \\

