ha042@BDAA-dev-dsvm-gpu-vm609:~/pytorch-pretrained-BERT$ time bash experiments/heads_ablation.sh MNLI
Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.
22:47:10-INFO: device: cuda n_gpu: 2, distributed training: False, 16-bits training: False
22:47:10-INFO: loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ha042/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
22:47:12-INFO: loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /home/ha042/.pytorch_pretrained_bert/distributed_-1/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba
22:47:12-INFO: extracting archive file /home/ha042/.pytorch_pretrained_bert/distributed_-1/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpxa_h2w3f
22:47:16-INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

22:47:19-INFO: Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
22:47:19-INFO: Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
22:47:22-INFO: loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /home/ha042/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba
22:47:22-INFO: extracting archive file /home/ha042/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpuu_ohhay
22:47:26-INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

22:47:29-INFO: ***** Running evaluation *****
22:47:29-INFO:   Num examples = 4561
22:47:29-INFO:   Batch size = 32
Evaluating: 100%|█████████████████████████████████████████████████████████████████████████| 143/143 [00:23<00:00,  6.17it/s]
22:47:52-INFO: ***** Eval results *****
22:47:52-INFO:   Accuracy = 0.642622231966674
22:47:52-INFO:   eval_accuracy = 0.642622231966674
22:47:52-INFO:   eval_loss = 0.9110127991729683
22:47:52-INFO:   global_step = 0
22:47:52-INFO:   inference_time = 23.064669370651245
22:47:52-INFO:   loss = None
0.642622231966674

Layer \textbackslash~Head & 1 & 2 & 3 & 4 \\
1 & 0.00044 & -0.00307 & -0.00241 & -0.00219 & 0.00088 & -0.00329 & -0.00110 & -0.00044 & -0.00570 & 0.00132 & 0.00066 & 0.00044 \\
2 & -0.00022 & 0.00132 & 0.00088 & -0.00110 & -0.00088 & -0.00307 & 0.00066 & -0.00285 & 0.00044 & -0.00066 & 0.00088 & -0.00022 \\
3 & 0.00044 & -0.00110 & -0.00263 & -0.00373 & -0.00022 & 0.00000 & -0.00658 & -0.00088 & -0.00066 & 0.00022 & -0.00066 & 0.00175 \\
4 & -0.00175 & -0.00044 & -0.00044 & -0.00153 & -0.00088 & -0.00022 & -0.00088 & -0.00175 & 0.00022 & -0.00175 & -0.00373 & -0.00132 \\
5 & 0.00000 & -0.00044 & -0.00460 & 0.00022 & 0.00044 & 0.00000 & -0.00088 & 0.00219 & -0.00066 & 0.00132 & 0.00000 & 0.00110 \\
6 & 0.00175 & 0.00022 & 0.00175 & 0.00088 & 0.00110 & -0.00241 & -0.00153 & 0.00110 & -0.00197 & -0.00066 & -0.00219 & 0.00197 \\
7 & 0.00285 & -0.00088 & -0.00110 & -0.00022 & -0.00175 & 0.00088 & -0.00066 & -0.00132 & -0.00307 & -0.00066 & -0.00132 & -0.00088 \\
8 & 0.00066 & -0.00088 & -0.00022 & -0.00022 & 0.00110 & -0.00088 & 0.00000 & 0.00088 & -0.00044 & 0.00066 & -0.00022 & -0.00241 \\
9 & -0.00263 & -0.00044 & -0.00044 & -0.00241 & 0.00153 & -0.00197 & -0.00022 & -0.00175 & -0.00066 & 0.00000 & -0.00153 & 0.00022 \\
10 & -0.00132 & -0.00153 & 0.00066 & 0.00000 & -0.00285 & -0.00110 & -0.00022 & 0.00000 & 0.00745 & -0.00022 & -0.00175 & -0.02039 \\
11 & 0.00175 & -0.00197 & -0.00022 & -0.00417 & 0.00022 & 0.00022 & -0.00110 & 0.00022 & -0.00395 & -0.00241 & -0.01535 & 0.00044 \\
12 & -0.00460 & 0.00197 & 0.00767 & 0.00197 & 0.00044 & -0.00614 & -0.00504 & 0.00000 & 0.00110 & -0.00022 & 0.00132 & -0.00636 \\

real	108m22.416s
user	100m13.090s
sys	9m35.930s

